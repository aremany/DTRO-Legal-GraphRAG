"""
ì›ë³¸ TXT íŒŒì¼ì„ ChromaDBì— ì„ë² ë”©
- 144ê°œ ì›ë³¸ ê·œì • íŒŒì¼ ì²˜ë¦¬
- 800ì ì²­í¬ ë¶„í•  (200ì ì˜¤ë²„ë©)
- GraphRAG ë©”íƒ€ë°ì´í„° ë³´ê°•
"""

import os
import json
import re
import warnings
import ssl
import urllib3
from pathlib import Path
from typing import List, Dict, Any
from tqdm import tqdm

# SSL ìš°íšŒ
os.environ['HF_HUB_DISABLE_SSL_VERIFY'] = '1'
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
ssl._create_default_https_context = ssl._create_unverified_context

import chromadb
from sentence_transformers import SentenceTransformer

# ì„¤ì •
BASE_DIR = Path(__file__).parent / "data"
CHROMA_PATH = "chroma_db_fulltext"  # ìƒˆë¡œìš´ ì»¬ë ‰ì…˜
COLLECTION_NAME = "dtro_fulltext_v1"
MODEL_NAME = "BAAI/bge-m3"
RULE_MD_PATH = "rule.md"

CHUNK_SIZE = 800
CHUNK_OVERLAP = 200

print("="*60)
print("ğŸ“š ì›ë³¸ TXT íŒŒì¼ ì„ë² ë”© ì‹œì‘")
print("="*60)

# ëª¨ë¸ ë¡œë“œ
print(f"ğŸ“¦ ëª¨ë¸ ë¡œë”©: {MODEL_NAME}")
model = SentenceTransformer(MODEL_NAME)
model.max_seq_length = 8192
print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (ì°¨ì›: {model.get_sentence_embedding_dimension()})")

# ChromaDB ì´ˆê¸°í™”
print(f"ğŸ’¾ ChromaDB ì´ˆê¸°í™”: {CHROMA_PATH}")
client = chromadb.PersistentClient(path=CHROMA_PATH)
collection = client.get_or_create_collection(name=COLLECTION_NAME)
print("âœ… ChromaDB ì¤€ë¹„ ì™„ë£Œ")


def parse_rule_md(rule_path: str) -> Dict[str, str]:
    """rule.md íŒŒì‹±í•˜ì—¬ ì¹´í…Œê³ ë¦¬ ë§¤í•‘ ë°˜í™˜"""
    mappings = {}
    
    if not os.path.exists(rule_path):
        print(f"âš ï¸  {rule_path} íŒŒì¼ ì—†ìŒ")
        return mappings
    
    with open(rule_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # ì¹´í…Œê³ ë¦¬ë³„ ê·¸ë£¹ íŒŒì‹±
    pattern = r'## (\d+_[^\n]+)\n(.*?)(?=\n##|\Z)'
    matches = re.findall(pattern, content, re.DOTALL)
    
    for category, groups_text in matches:
        # ê·¸ë£¹ë³„ íŒŒì¼ íŒŒì‹±
        group_pattern = r'### (ê·¸ë£¹\d+)\n```\n(.*?)\n```'
        group_matches = re.findall(group_pattern, groups_text, re.DOTALL)
        
        for group, files_text in group_matches:
            files = [f.strip() for f in files_text.split('\n') if f.strip()]
            for filename in files:
                mappings[filename] = {
                    'category': category,
                    'group': group
                }
    
    return mappings


def chunk_text(text: str, chunk_size: int = CHUNK_SIZE, overlap: int = CHUNK_OVERLAP) -> List[str]:
    """í…ìŠ¤íŠ¸ë¥¼ ì˜¤ë²„ë©ì´ ìˆëŠ” ì²­í¬ë¡œ ë¶„í• """
    if len(text) <= chunk_size:
        return [text]
    
    chunks = []
    start = 0
    
    while start < len(text):
        end = start + chunk_size
        
        # ì²­í¬ ì¶”ì¶œ
        chunk = text[start:end]
        
        # ë§ˆì§€ë§‰ ì²­í¬ê°€ ì•„ë‹ˆë©´ ë¬¸ì¥ ê²½ê³„ì—ì„œ ìë¥´ê¸°
        if end < len(text):
            # ë§ˆì§€ë§‰ ë§ˆì¹¨í‘œ ì°¾ê¸°
            last_period = chunk.rfind('.')
            last_newline = chunk.rfind('\n')
            
            cut_point = max(last_period, last_newline)
            if cut_point > chunk_size * 0.5:  # ë„ˆë¬´ ì§§ì•„ì§€ì§€ ì•Šë„ë¡
                chunk = chunk[:cut_point + 1]
                end = start + len(chunk)
        
        chunks.append(chunk.strip())
        
        # ë‹¤ìŒ ì‹œì‘ì  (ì˜¤ë²„ë© ì ìš©)
        start = end - overlap
        
        # ë¬´í•œ ë£¨í”„ ë°©ì§€
        if start <= 0 or start >= len(text):
            break
    
    return chunks


def load_txt_files(base_dir: Path, mappings: Dict[str, Dict]) -> List[Dict[str, Any]]:
    """ëª¨ë“  TXT íŒŒì¼ ë¡œë“œ ë° ì²­í‚¹"""
    documents = []
    
    # ì¹´í…Œê³ ë¦¬ í´ë” íƒìƒ‰
    category_folders = [
        "01_ì¡°ì§ê²½ì˜", "02_ì¸ì‚¬ë…¸ë¬´", "03_ì¬ë¬´íšŒê³„", "04_ìš´ì „ìš´í–‰",
        "05_ì°¨ëŸ‰ê²€ìˆ˜", "06_ì„ ë¡œê¶¤ë„", "07_ì „ê¸°ì„¤ë¹„", "08_ì‹ í˜¸í†µì‹ ",
        "09_ê±´ì¶•ê¸°ê³„", "10_ì•ˆì „ë³´ì•ˆ", "11_ê³ ê°ì„œë¹„ìŠ¤", "12_ê°ì‚¬ì²­ë ´",
        "13_ì‚¬ë¬´í–‰ì •", "14_ì—°êµ¬ê¸°íš", "15_ê¸°íƒ€íŠ¹ìˆ˜"
    ]
    
    total_files = 0
    total_chunks = 0
    
    for category in tqdm(category_folders, desc="ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬"):
        category_path = base_dir / category
        
        if not category_path.exists():
            continue
        
        # ê·¸ë£¹ í´ë” íƒìƒ‰
        for group_path in category_path.iterdir():
            if not group_path.is_dir():
                continue
            
            group_name = group_path.name
            
            # TXT íŒŒì¼ ì²˜ë¦¬
            for txt_file in group_path.glob("*.txt"):
                filename = txt_file.name
                
                # ë§¤í•‘ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                file_info = mappings.get(filename, {
                    'category': category,
                    'group': group_name
                })
                
                try:
                    # íŒŒì¼ ì½ê¸°
                    with open(txt_file, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # ì²­í‚¹
                    chunks = chunk_text(content)
                    total_files += 1
                    total_chunks += len(chunks)
                    
                    # ê° ì²­í¬ë¥¼ ë¬¸ì„œë¡œ ì¶”ê°€
                    for i, chunk in enumerate(chunks):
                        doc = {
                            'id': f"{filename}_{i}",
                            'text': chunk,
                            'metadata': {
                                'source_file': filename,
                                'category': file_info['category'],
                                'group': file_info['group'],
                                'chunk_index': i,
                                'total_chunks': len(chunks),
                                'file_path': str(txt_file)
                            }
                        }
                        documents.append(doc)
                
                except Exception as e:
                    print(f"\nâš ï¸  íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {filename} - {e}")
    
    print(f"\nğŸ“Š ì²˜ë¦¬ ì™„ë£Œ:")
    print(f"   - íŒŒì¼ ìˆ˜: {total_files}ê°œ")
    print(f"   - ì²­í¬ ìˆ˜: {total_chunks}ê°œ")
    
    return documents


def upload_to_chromadb(documents: List[Dict[str, Any]], batch_size: int = 64):
    """ChromaDBì— ì„ë² ë”© ë° ì—…ë¡œë“œ"""
    print(f"\nğŸš€ ì„ë² ë”© ìƒì„± ë° ì—…ë¡œë“œ ì‹œì‘ (ë°°ì¹˜ í¬ê¸°: {batch_size})")
    
    total = len(documents)
    success = 0
    failed = 0
    
    for i in tqdm(range(0, total, batch_size), desc="ì„ë² ë”© ë°°ì¹˜"):
        batch = documents[i:i+batch_size]
        
        try:
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            texts = [doc['text'] for doc in batch]
            ids = [doc['id'] for doc in batch]
            metadatas = [doc['metadata'] for doc in batch]
            
            # ì„ë² ë”© ìƒì„±
            embeddings = model.encode(
                texts,
                normalize_embeddings=True,
                show_progress_bar=False,
                convert_to_numpy=True
            )
            
            # numpy arrayë¥¼ listë¡œ ë³€í™˜
            if hasattr(embeddings, 'tolist'):
                embeddings = embeddings.tolist()
            
            # ChromaDB ì—…ë¡œë“œ
            collection.add(
                embeddings=embeddings,
                documents=texts,
                metadatas=metadatas,
                ids=ids
            )
            
            success += len(batch)
        
        except Exception as e:
            print(f"\nâš ï¸  ë°°ì¹˜ {i//batch_size + 1} ì‹¤íŒ¨: {e}")
            failed += len(batch)
            
            # ê°œë³„ ì¬ì‹œë„
            for doc in batch:
                try:
                    emb = model.encode([doc['text']], normalize_embeddings=True, convert_to_numpy=True)
                    if hasattr(emb, 'tolist'):
                        emb = emb.tolist()
                    
                    collection.add(
                        embeddings=emb,
                        documents=[doc['text']],
                        metadatas=[doc['metadata']],
                        ids=[doc['id']]
                    )
                    success += 1
                    failed -= 1
                except:
                    print(f"   âŒ {doc['id']} ì‹¤íŒ¨")
    
    print(f"\nğŸ“Š ì—…ë¡œë“œ í†µê³„:")
    print(f"   âœ… ì„±ê³µ: {success}ê°œ")
    print(f"   âŒ ì‹¤íŒ¨: {failed}ê°œ")
    print(f"   ğŸ“ˆ ì„±ê³µë¥ : {(success/(success+failed)*100):.1f}%")


def main():
    # 1. rule.md íŒŒì‹±
    print("\nğŸ“– rule.md íŒŒì‹± ì¤‘...")
    mappings = parse_rule_md(RULE_MD_PATH)
    print(f"âœ… {len(mappings)}ê°œ íŒŒì¼ ë§¤í•‘ ì •ë³´ ë¡œë“œ")
    
    # 2. TXT íŒŒì¼ ë¡œë“œ ë° ì²­í‚¹
    print(f"\nğŸ“‚ TXT íŒŒì¼ ë¡œë“œ ì¤‘: {BASE_DIR}")
    documents = load_txt_files(BASE_DIR, mappings)
    
    if not documents:
        print("âŒ ì²˜ë¦¬í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤!")
        return
    
    # 3. ChromaDB ì—…ë¡œë“œ
    upload_to_chromadb(documents)
    
    # 4. ê²€ì¦
    doc_count = collection.count()
    print(f"\nâœ… ChromaDB ì €ì¥ ì™„ë£Œ:")
    print(f"   - ê²½ë¡œ: {CHROMA_PATH}")
    print(f"   - ì»¬ë ‰ì…˜: {COLLECTION_NAME}")
    print(f"   - ë¬¸ì„œ ìˆ˜: {doc_count}ê°œ")
    
    print("\n" + "="*60)
    print("ğŸ‰ ì›ë³¸ TXT íŒŒì¼ ì„ë² ë”© ì™„ë£Œ!")
    print("="*60)


if __name__ == "__main__":
    main()
