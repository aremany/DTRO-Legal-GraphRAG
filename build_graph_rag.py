import json
import os
import pickle
import time
import networkx as nx
import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
from tqdm import tqdm
import urllib3
import ssl
from text_processor import TextProcessor

# SSL ì¸ì¦ì„œ ê²€ì¦ ë¬´ì‹œ ì„¤ì •
os.environ['HF_HUB_DISABLE_SSL_VERIFY'] = '1'
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
try:
    _create_unverified_https_context = ssl._create_unverified_context
except AttributeError:
    pass
else:
    ssl._create_default_https_context = _create_unverified_https_context

# ì„¤ì •
JSON_PATH = "total_graph.json"
MAPPING_PATH = "source_file_mapping.json"
CHROMA_PATH = "chroma_db"
GRAPH_PKL_PATH = "graph_data.pkl"
MODEL_NAME = "BAAI/bge-m3"

# ì²­í‚¹ ì„¤ì •
ENABLE_CHUNKING = True          # ì²­í‚¹ í™œì„±í™” ì—¬ë¶€
CHUNKING_METHOD = 'semantic'    # 'semantic' ë˜ëŠ” 'fixed'
CHUNK_THRESHOLD = 1000          # ì´ ê¸¸ì´ ì´ìƒì´ë©´ ì²­í‚¹

# ì„ë² ë”© ì„¤ì •
BATCH_SIZE = 64                 # ë°°ì¹˜ í¬ê¸° (êµ¬ì„ê¸°: 100, ê¸°ì¡´: 32 â†’ ì¤‘ê°„ê°’ 64)
MAX_SEQ_LENGTH = 8192           # BGE-M3 ìµœëŒ€ í† í° ê¸¸ì´

# í†µê³„
stats = {
    'total_nodes': 0,
    'chunked_nodes': 0,
    'total_chunks': 0,
    'embedding_success': 0,
    'embedding_failed': 0,
    'start_time': 0,
    'end_time': 0
}

def load_json(path):
    with open(path, 'r', encoding='utf-8') as f:
        return json.load(f)

def build_networkx_graph(data):
    print("ğŸ•¸ï¸  NetworkX ê·¸ë˜í”„ ìƒì„± ì¤‘...")
    G = nx.DiGraph()
    
    # ë…¸ë“œ ì¶”ê°€
    for node in data['nodes']:
        G.add_node(node['id'], **node)
        
    # ì—£ì§€ ì¶”ê°€
    for edge in data['edges']:
        G.add_edge(
            edge['source'], 
            edge['target'], 
            relationship=edge['relationship'],
            **edge.get('properties', {})
        )
        
    print(f"   - ë…¸ë“œ ìˆ˜: {G.number_of_nodes()}")
    print(f"   - ì—£ì§€ ìˆ˜: {G.number_of_edges()}")
    
    # ì €ì¥
    with open(GRAPH_PKL_PATH, 'wb') as f:
        pickle.dump(G, f)
    print(f"âœ… ê·¸ë˜í”„ ì €ì¥ ì™„ë£Œ: {GRAPH_PKL_PATH}")
    return G

def find_source_info(filename, mapping):
    """íŒŒì¼ëª…ìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ì™€ ì „ì²´ ê²½ë¡œë¥¼ ì°¾ìŒ"""
    if not filename:
        return "Unknown", "Unknown", "Unknown"
        
    target_file = filename.strip()
    
    for json_group, files in mapping.items():
        for full_path in files:
            if os.path.basename(full_path) == target_file:
                parts = full_path.replace("\\", "/").split("/")
                
                try:
                    idx = parts.index("ë¶„ë¥˜ì‘ì—…")
                    category = parts[idx+1]
                    group = parts[idx+2] if idx+2 < len(parts)-1 else "N/A"
                    return category, group, full_path
                except ValueError:
                    return "Unknown", "Unknown", full_path
                    
    return "Unknown", "Unknown", "Unknown"

def serialize_node(node, mapping, text_processor=None):
    """
    ë…¸ë“œë¥¼ ì„ë² ë”©ìš© í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ì²­í‚¹ ì§€ì›)
    
    Returns:
        List[Tuple[str, dict]]: (í…ìŠ¤íŠ¸, ë©”íƒ€ë°ì´í„°) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
    """
    props = node.get('properties', {})
    source_file = props.get('source_file', '')
    
    category, group, full_path = find_source_info(source_file, mapping)
    
    # ê¸°ë³¸ ë©”íƒ€ë°ì´í„°
    base_metadata = {
        "node_id": node['id'],
        "type": node.get('type', 'N/A'),
        "label": node.get('label', 'N/A'),
        "category": category,
        "group": group,
        "source_file": source_file
    }
    
    # ê¸°ë³¸ í…ìŠ¤íŠ¸ (ì„¤ëª… ì œì™¸)
    base_text_parts = [
        f"[ì¹´í…Œê³ ë¦¬] {category} > {group}",
        f"[ë¬¸ì„œ] {source_file}",
        f"[íƒ€ì…] {node.get('type', 'N/A')}",
        f"[ì´ë¦„] {node.get('label', 'N/A')}"
    ]
    
    desc = props.get('description', '')
    
    # ì²­í‚¹ í•„ìš” ì—¬ë¶€ íŒë‹¨
    if ENABLE_CHUNKING and text_processor and desc and text_processor.should_chunk(desc, CHUNK_THRESHOLD):
        # ì²­í‚¹ ìˆ˜í–‰
        chunks = text_processor.chunk_node_description(desc, node['id'], method=CHUNKING_METHOD)
        stats['chunked_nodes'] += 1
        stats['total_chunks'] += len(chunks)
        
        # ê° ì²­í¬ì— ëŒ€í•´ í…ìŠ¤íŠ¸ ìƒì„±
        results = []
        for chunk in chunks:
            chunk_text_parts = base_text_parts.copy()
            chunk_text_parts.append(f"[ì„¤ëª… {chunk['chunk_index']+1}/{len(chunks)}] {chunk['content']}")
            
            # ë©”íƒ€ë°ì´í„°ì— ì²­í¬ ì •ë³´ ì¶”ê°€
            chunk_metadata = base_metadata.copy()
            chunk_metadata.update({
                'chunk_index': chunk['chunk_index'],
                'total_chunks': len(chunks),
                'chunking_method': chunk['method']
            })
            
            results.append(("\n".join(chunk_text_parts), chunk_metadata))
        
        return results
    else:
        # ì²­í‚¹ ë¶ˆí•„ìš” - ë‹¨ì¼ í…ìŠ¤íŠ¸ ë°˜í™˜
        text_parts = base_text_parts.copy()
        
        if desc:
            # ì „ì²˜ë¦¬ë§Œ ì ìš©
            cleaned_desc = text_processor.clean_text(desc) if text_processor else desc
            text_parts.append(f"[ì„¤ëª…] {cleaned_desc}")
        
        # ê¸°íƒ€ ì†ì„± ì¶”ê°€
        for k, v in props.items():
            if k not in ['source_file', 'description', 'cite_pages']:
                text_parts.append(f"[{k}] {v}")
        
        metadata = base_metadata.copy()
        metadata['chunked'] = False
        
        return [("\n".join(text_parts), metadata)]

def build_chroma_db(data, mapping):
    print(f"ğŸ’¾ ChromaDB êµ¬ì¶• ì‹œì‘ (ëª¨ë¸: {MODEL_NAME})...")
    print(f"   ğŸ§  ì²­í‚¹: {'í™œì„±í™”' if ENABLE_CHUNKING else 'ë¹„í™œì„±í™”'} ({CHUNKING_METHOD if ENABLE_CHUNKING else 'N/A'})")
    print(f"   ğŸ“¦ ë°°ì¹˜ í¬ê¸°: {BATCH_SIZE}")
    print(f"   ğŸ“ ìµœëŒ€ í† í°: {MAX_SEQ_LENGTH}")
    
    stats['start_time'] = time.time()
    stats['total_nodes'] = len(data['nodes'])
    
    # 1. ëª¨ë¸ ë¡œë“œ
    print("   ğŸ¤– ëª¨ë¸ ë¡œë”© ì¤‘...")
    model = SentenceTransformer(MODEL_NAME)
    model.max_seq_length = MAX_SEQ_LENGTH
    print(f"   âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (ì°¨ì›: {model.get_sentence_embedding_dimension()})")
    
    # 2. í…ìŠ¤íŠ¸ ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
    text_processor = TextProcessor() if ENABLE_CHUNKING else None
    
    # 3. DB ì´ˆê¸°í™”
    client = chromadb.PersistentClient(path=CHROMA_PATH)
    collection = client.get_or_create_collection(name="dtro_graph_v1")
    
    # 4. ë°ì´í„° ì¤€ë¹„ (ì²­í‚¹ ì ìš©)
    all_documents = []
    all_metadatas = []
    all_ids = []
    
    print("   ğŸ“ ë°ì´í„° ì „ì²˜ë¦¬ ë° ì²­í‚¹ ì¤‘...")
    for node in tqdm(data['nodes'], desc="ë…¸ë“œ ì²˜ë¦¬", unit="node"):
        # ë…¸ë“œë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ì²­í‚¹ í¬í•¨)
        text_metadata_pairs = serialize_node(node, mapping, text_processor)
        
        for idx, (text, meta) in enumerate(text_metadata_pairs):
            # ê³ ìœ  ID ìƒì„± (ì²­í¬ê°€ ì—¬ëŸ¬ ê°œì¸ ê²½ìš° _chunk_{idx} ì¶”ê°€)
            if len(text_metadata_pairs) > 1:
                doc_id = f"{node['id']}_chunk_{idx}"
            else:
                doc_id = node['id']
            
            all_documents.append(text)
            all_metadatas.append(meta)
            all_ids.append(doc_id)
    
    total = len(all_documents)
    print(f"   ğŸ“Š ì´ {total}ê°œ ë¬¸ì„œ ìƒì„± (ì›ë³¸ ë…¸ë“œ: {stats['total_nodes']}ê°œ)")
    if stats['chunked_nodes'] > 0:
        print(f"   âœ‚ï¸  ì²­í‚¹ëœ ë…¸ë“œ: {stats['chunked_nodes']}ê°œ â†’ {stats['total_chunks']}ê°œ ì²­í¬")
    
    # 5. ì„ë² ë”© ë° ì €ì¥ (ë°°ì¹˜ ì²˜ë¦¬ + ì‹¤íŒ¨ ì²˜ë¦¬)
    print(f"   ğŸš€ ì„ë² ë”© ìƒì„± ë° ì €ì¥ ì‹œì‘...")
    
    for i in tqdm(range(0, total, BATCH_SIZE), desc="ì„ë² ë”© ë°°ì¹˜", unit="batch"):
        batch_docs = all_documents[i:i+BATCH_SIZE]
        batch_metas = all_metadatas[i:i+BATCH_SIZE]
        batch_ids = all_ids[i:i+BATCH_SIZE]
        
        try:
            # ì„ë² ë”© ìƒì„±
            embeddings = model.encode(
                batch_docs, 
                normalize_embeddings=True,
                show_progress_bar=False,  # tqdmê³¼ ì¤‘ë³µ ë°©ì§€
                convert_to_numpy=True  # numpy arrayë¡œ ë³€í™˜
            )
            
            # numpy arrayë¥¼ listë¡œ ë³€í™˜ (ChromaDB í˜¸í™˜ì„±)
            if hasattr(embeddings, 'tolist'):
                embeddings = embeddings.tolist()
            # Tensor ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ì²˜ë¦¬
            elif isinstance(embeddings, list) and len(embeddings) > 0:
                import torch
                if isinstance(embeddings[0], torch.Tensor):
                    embeddings = [emb.cpu().numpy().tolist() for emb in embeddings]
            
            # DB ì¶”ê°€
            collection.add(
                embeddings=embeddings,
                documents=batch_docs,
                metadatas=batch_metas,
                ids=batch_ids
            )
            
            stats['embedding_success'] += len(batch_docs)
            
        except Exception as e:
            print(f"\n   âš ï¸  ë°°ì¹˜ {i//BATCH_SIZE + 1} ì„ë² ë”© ì‹¤íŒ¨: {e}")
            stats['embedding_failed'] += len(batch_docs)
            
            # ê°œë³„ ì²˜ë¦¬ ì‹œë„ (ì‹¤íŒ¨ ë³µêµ¬)
            for j, (doc, meta, doc_id) in enumerate(zip(batch_docs, batch_metas, batch_ids)):
                try:
                    emb = model.encode([doc], normalize_embeddings=True, show_progress_bar=False, convert_to_numpy=True)
                    if hasattr(emb, 'tolist'):
                        emb = emb.tolist()
                    elif isinstance(emb, list) and len(emb) > 0:
                        import torch
                        if isinstance(emb[0], torch.Tensor):
                            emb = [e.cpu().numpy().tolist() for e in emb]
                    
                    collection.add(
                        embeddings=emb,
                        documents=[doc],
                        metadatas=[meta],
                        ids=[doc_id]
                    )
                    stats['embedding_success'] += 1
                    stats['embedding_failed'] -= 1
                except:
                    print(f"      âŒ ë¬¸ì„œ {doc_id} ì„ë² ë”© ì‹¤íŒ¨ (ê±´ë„ˆëœ€)")
    
    stats['end_time'] = time.time()
    
    print(f"\nâœ… ChromaDB ì €ì¥ ì™„ë£Œ: {CHROMA_PATH}")
    print_statistics()

def print_statistics():
    """í†µê³„ ì¶œë ¥"""
    elapsed = stats['end_time'] - stats['start_time']
    
    print("\n" + "="*60)
    print("ğŸ“Š êµ¬ì¶• í†µê³„")
    print("="*60)
    print(f"ğŸ• ì²˜ë¦¬ ì‹œê°„: {elapsed:.1f}ì´ˆ")
    print(f"ğŸ“¦ ì›ë³¸ ë…¸ë“œ: {stats['total_nodes']}ê°œ")
    
    if stats['chunked_nodes'] > 0:
        print(f"âœ‚ï¸  ì²­í‚¹ ì ìš©: {stats['chunked_nodes']}ê°œ ë…¸ë“œ â†’ {stats['total_chunks']}ê°œ ì²­í¬")
        avg_chunks = stats['total_chunks'] / stats['chunked_nodes']
        print(f"   í‰ê·  ì²­í¬ ìˆ˜: {avg_chunks:.1f}ê°œ/ë…¸ë“œ")
    
    total_docs = stats['embedding_success'] + stats['embedding_failed']
    print(f"ğŸ“„ ì´ ë¬¸ì„œ ìˆ˜: {total_docs}ê°œ")
    print(f"âœ… ì„ë² ë”© ì„±ê³µ: {stats['embedding_success']}ê°œ")
    print(f"âŒ ì„ë² ë”© ì‹¤íŒ¨: {stats['embedding_failed']}ê°œ")
    
    if total_docs > 0:
        success_rate = (stats['embedding_success'] / total_docs) * 100
        print(f"ğŸ“ˆ ì„±ê³µë¥ : {success_rate:.1f}%")
    
    if stats['embedding_success'] > 0 and elapsed > 0:
        throughput = stats['embedding_success'] / elapsed
        print(f"âš¡ ì²˜ë¦¬ ì†ë„: {throughput:.1f} docs/sec")
    
    print("="*60)

def test_gpu_performance(model, test_size=10):
    """GPU ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
    print(f"\nğŸ§ª GPU ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ({test_size}ê°œ ìƒ˜í”Œ)...")
    
    test_texts = [
        f"í…ŒìŠ¤íŠ¸ ë¬¸ì¥ {i}: BGE-M3 ëª¨ë¸ì˜ GPU ê°€ì† ì„±ëŠ¥ì„ ì¸¡ì •í•˜ê¸° ìœ„í•œ ìƒ˜í”Œ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤. "
        f"ì´ í…ìŠ¤íŠ¸ëŠ” ì‹¤ì œ ë…¸ë“œ ì„¤ëª…ê³¼ ìœ ì‚¬í•œ ê¸¸ì´ë¡œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤." * 5
        for i in range(test_size)
    ]
    
    start = time.time()
    embeddings = model.encode(test_texts, normalize_embeddings=True, show_progress_bar=False)
    end = time.time()
    
    elapsed = end - start
    throughput = test_size / elapsed if elapsed > 0 else 0
    
    print(f"   â±ï¸  ì‹œê°„: {elapsed:.2f}ì´ˆ")
    print(f"   ğŸš€ ì†ë„: {throughput:.1f} texts/sec")
    print(f"   ğŸ“Š í‰ê· : {(elapsed/test_size)*1000:.1f}ms/text")

def main():
    if not os.path.exists(JSON_PATH):
        print(f"âŒ íŒŒì¼ ì—†ìŒ: {JSON_PATH}")
        return
    
    print("="*60)
    print("ğŸš€ GraphRAG êµ¬ì¶• ì‹œì‘ (ê³ ë„í™” ë²„ì „)")
    print("="*60)
        
    # ë°ì´í„° ë¡œë“œ
    print("\nğŸ“‚ ë°ì´í„° ë¡œë”© ì¤‘...")
    graph_data = load_json(JSON_PATH)
    mapping_data = load_json(MAPPING_PATH) if os.path.exists(MAPPING_PATH) else {}
    print(f"   âœ… ê·¸ë˜í”„ ë°ì´í„°: ë…¸ë“œ {len(graph_data['nodes'])}ê°œ, ì—£ì§€ {len(graph_data['edges'])}ê°œ")
    
    # 1. NetworkX ê·¸ë˜í”„ ë¹Œë“œ
    print("\n" + "="*60)
    build_networkx_graph(graph_data)
    
    # 2. ChromaDB ë¹Œë“œ (ì²­í‚¹, ì§„í–‰ë¥ , í†µê³„ í¬í•¨)
    print("\n" + "="*60)
    build_chroma_db(graph_data, mapping_data)
    
    print("\n" + "="*60)
    print("ğŸ‰ ëª¨ë“  êµ¬ì¶• ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    print("="*60)

if __name__ == "__main__":
    main()
