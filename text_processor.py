"""
GraphRAGìš© í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° ì²­í‚¹ ëª¨ë“ˆ
êµ¬ì„ê¸° ë²„ì „ì˜ ì¥ì ì„ GraphRAGì— ì´ì‹
"""

import re
from typing import List, Dict, Any


class TextProcessor:
    """ë…¸ë“œ ì„¤ëª…ë¬¸ì„ ì²­í‚¹í•˜ê³  ì „ì²˜ë¦¬í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, 
                 chunk_size: int = 800,
                 overlap: int = 100,
                 min_chunk_size: int = 200,
                 max_chunk_size: int = 1500):
        """
        ì´ˆê¸°í™”
        
        Args:
            chunk_size: ê³ ì • ê¸¸ì´ ì²­í‚¹ ì‹œ ê¸°ë³¸ í¬ê¸°
            overlap: ì²­í¬ ê°„ ê²¹ì¹¨ í¬ê¸°
            min_chunk_size: ì˜ë¯¸ ì²­í‚¹ ì‹œ ìµœì†Œ í¬ê¸°
            max_chunk_size: ì˜ë¯¸ ì²­í‚¹ ì‹œ ìµœëŒ€ í¬ê¸°
        """
        self.chunk_size = chunk_size
        self.overlap = overlap
        self.min_chunk_size = min_chunk_size
        self.max_chunk_size = max_chunk_size
        
        print(f"ğŸ“ í…ìŠ¤íŠ¸ ì²˜ë¦¬ê¸° ì´ˆê¸°í™”")
        print(f"   ğŸ“ ì²­í¬ í¬ê¸°: {chunk_size} (ê²¹ì¹¨: {overlap})")
        print(f"   ğŸ§  ì˜ë¯¸ ì²­í‚¹ ë²”ìœ„: {min_chunk_size}~{max_chunk_size}ì")
    
    def clean_text(self, text: str) -> str:
        """
        í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        
        Args:
            text: ì›ë³¸ í…ìŠ¤íŠ¸
            
        Returns:
            ì •ì œëœ í…ìŠ¤íŠ¸
        """
        if not text:
            return ""
        
        # 1. ì—¬ëŸ¬ ê°œì˜ ê³µë°±ì„ í•˜ë‚˜ë¡œ
        text = re.sub(r'[ \t]+', ' ', text)
        
        # 2. ì—¬ëŸ¬ ê°œì˜ ì¤„ë°”ê¿ˆì„ ìµœëŒ€ 2ê°œë¡œ
        text = re.sub(r'\n{3,}', '\n\n', text)
        
        # 3. ê° ì¤„ì˜ ì•ë’¤ ê³µë°± ì œê±°
        lines = [line.strip() for line in text.split('\n')]
        text = '\n'.join(line for line in lines if line)
        
        # 4. ì „ì²´ ì•ë’¤ ê³µë°± ì œê±°
        return text.strip()
    
    def _detect_paragraph_boundaries(self, text: str) -> List[int]:
        """
        ë¬¸ë‹¨ ê²½ê³„ ê°ì§€ (ì˜ë¯¸ ì²­í‚¹ìš©)
        
        Args:
            text: ë¶„ì„í•  í…ìŠ¤íŠ¸
            
        Returns:
            ë¬¸ë‹¨ ê²½ê³„ ìœ„ì¹˜ ë¦¬ìŠ¤íŠ¸
        """
        boundaries = [0]
        
        # ë¬¸ë‹¨ êµ¬ë¶„ íŒ¨í„´ë“¤
        paragraph_patterns = [
            r'\n\n+',          # ë¹ˆ ì¤„
            r'\n\d+\.',        # ë²ˆí˜¸ ë§¤ê¸°ê¸° (1., 2., ...)
            r'\n[ê°€-í£]\.',    # í•œê¸€ ë¦¬ìŠ¤íŠ¸ (ê°€., ë‚˜., ...)
            r'\n-\s',          # í•˜ì´í”ˆ ë¦¬ìŠ¤íŠ¸
            r'\nâ€¢\s',          # ë¶ˆë¦¿ í¬ì¸íŠ¸
            r'\n\[.*?\]',      # ëŒ€ê´„í˜¸ ì„¹ì…˜
            r'\nì œ\d+ì¥',      # ì¥ êµ¬ë¶„
            r'\nì œ\d+ì ˆ',      # ì ˆ êµ¬ë¶„
            r'\n\d+\)\s'       # ë²ˆí˜¸ + ê´„í˜¸ (1) 2) ...)
        ]
        
        # ëª¨ë“  íŒ¨í„´ ì ìš©
        for pattern in paragraph_patterns:
            for match in re.finditer(pattern, text):
                pos = match.start()
                if pos > 0 and pos not in boundaries:
                    boundaries.append(pos)
        
        # ë¬¸ì¥ ë íŒ¨í„´ (.!? ë’¤ì— ì¤„ë°”ê¿ˆ)
        sentence_end_pattern = r'[.!?]\s*\n'
        for match in re.finditer(sentence_end_pattern, text):
            pos = match.end()
            if pos < len(text) - 1 and pos not in boundaries:
                boundaries.append(pos)
        
        # ë ìœ„ì¹˜ ì¶”ê°€
        boundaries.append(len(text))
        
        return sorted(list(set(boundaries)))
    
    def create_semantic_chunks(self, text: str, node_id: str) -> List[Dict[str, Any]]:
        """
        ì˜ë¯¸ ê¸°ë°˜ ì²­í‚¹ (ë¬¸ë‹¨ êµ¬ì¡° ì¸ì‹)
        
        Args:
            text: ì²­í‚¹í•  í…ìŠ¤íŠ¸
            node_id: ë…¸ë“œ ID
            
        Returns:
            ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        # ì „ì²˜ë¦¬
        text = self.clean_text(text)
        
        # ì§§ìœ¼ë©´ ê·¸ëƒ¥ ë°˜í™˜
        if len(text) <= self.max_chunk_size:
            return [{
                'content': text,
                'node_id': node_id,
                'chunk_index': 0,
                'start_pos': 0,
                'end_pos': len(text),
                'method': 'semantic_single'
            }]
        
        # ë¬¸ë‹¨ ê²½ê³„ ê°ì§€
        boundaries = self._detect_paragraph_boundaries(text)
        
        chunks = []
        current_chunk = ""
        chunk_index = 0
        start_pos = 0
        
        for i in range(len(boundaries) - 1):
            segment_start = boundaries[i]
            segment_end = boundaries[i + 1]
            segment = text[segment_start:segment_end].strip()
            
            if not segment:
                continue
            
            # í˜„ì¬ ì²­í¬ì— ì„¸ê·¸ë¨¼íŠ¸ ì¶”ê°€ ì‹œë„
            potential_chunk = (current_chunk + "\n" + segment).strip()
            
            if len(potential_chunk) <= self.max_chunk_size:
                # ìµœëŒ€ í¬ê¸° ì´í•˜ë©´ ê³„ì† ì¶”ê°€
                current_chunk = potential_chunk
            else:
                # ìµœëŒ€ í¬ê¸° ì´ˆê³¼
                if current_chunk and len(current_chunk) >= self.min_chunk_size:
                    # í˜„ì¬ ì²­í¬ ì €ì¥
                    chunks.append({
                        'content': current_chunk,
                        'node_id': node_id,
                        'chunk_index': chunk_index,
                        'start_pos': start_pos,
                        'end_pos': start_pos + len(current_chunk),
                        'method': 'semantic'
                    })
                    chunk_index += 1
                    start_pos = segment_start
                
                # ìƒˆ ì²­í¬ ì‹œì‘
                current_chunk = segment
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì²˜ë¦¬
        if current_chunk:
            if len(current_chunk) >= self.min_chunk_size or not chunks:
                # ìµœì†Œ í¬ê¸° ì´ìƒì´ê±°ë‚˜ ìœ ì¼í•œ ì²­í¬ì¸ ê²½ìš°
                chunks.append({
                    'content': current_chunk,
                    'node_id': node_id,
                    'chunk_index': chunk_index,
                    'start_pos': start_pos,
                    'end_pos': len(text),
                    'method': 'semantic'
                })
            elif chunks:
                # ë„ˆë¬´ ì‘ìœ¼ë©´ ì´ì „ ì²­í¬ì— ë³‘í•©
                chunks[-1]['content'] += "\n" + current_chunk
                chunks[-1]['end_pos'] = len(text)
        
        return chunks
    
    def create_fixed_length_chunks(self, text: str, node_id: str) -> List[Dict[str, Any]]:
        """
        ê³ ì • ê¸¸ì´ ì²­í‚¹ (Overlap í¬í•¨)
        
        Args:
            text: ì²­í‚¹í•  í…ìŠ¤íŠ¸
            node_id: ë…¸ë“œ ID
            
        Returns:
            ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        # ì „ì²˜ë¦¬
        text = self.clean_text(text)
        
        # ì§§ìœ¼ë©´ ê·¸ëƒ¥ ë°˜í™˜
        if len(text) <= self.chunk_size:
            return [{
                'content': text,
                'node_id': node_id,
                'chunk_index': 0,
                'start_pos': 0,
                'end_pos': len(text),
                'method': 'fixed_single'
            }]
        
        chunks = []
        start = 0
        chunk_index = 0
        
        while start < len(text):
            end = min(start + self.chunk_size, len(text))
            
            # ë¬¸ì¥ ëì—ì„œ ìë¥´ê¸° ì‹œë„
            if end < len(text):
                sentence_end = max(
                    text.rfind('.', start, end),
                    text.rfind('!', start, end),
                    text.rfind('?', start, end)
                )
                
                if sentence_end > start:
                    end = sentence_end + 1
                else:
                    # ë¬¸ì¥ ë ëª» ì°¾ìœ¼ë©´ ê³µë°±ì—ì„œ ìë¥´ê¸°
                    word_end = text.rfind(' ', start, end)
                    if word_end > start:
                        end = word_end
            
            chunk_content = text[start:end].strip()
            
            if chunk_content:
                chunks.append({
                    'content': chunk_content,
                    'node_id': node_id,
                    'chunk_index': chunk_index,
                    'start_pos': start,
                    'end_pos': end,
                    'method': 'fixed'
                })
                chunk_index += 1
            
            # ëì´ë©´ ì¤‘ë‹¨
            if end >= len(text):
                break
            
            # Overlap ì ìš©
            start = max(start + 1, end - self.overlap)
        
        return chunks
    
    def chunk_node_description(self, 
                               description: str, 
                               node_id: str,
                               method: str = 'semantic') -> List[Dict[str, Any]]:
        """
        ë…¸ë“œ ì„¤ëª…ì„ ì²­í‚¹
        
        Args:
            description: ë…¸ë“œ ì„¤ëª… í…ìŠ¤íŠ¸
            node_id: ë…¸ë“œ ID
            method: 'semantic' ë˜ëŠ” 'fixed'
            
        Returns:
            ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        if not description or not description.strip():
            return []
        
        if method == 'semantic':
            return self.create_semantic_chunks(description, node_id)
        else:
            return self.create_fixed_length_chunks(description, node_id)
    
    def should_chunk(self, text: str, threshold: int = 1000) -> bool:
        """
        í…ìŠ¤íŠ¸ê°€ ì²­í‚¹ì´ í•„ìš”í•œì§€ íŒë‹¨
        
        Args:
            text: í™•ì¸í•  í…ìŠ¤íŠ¸
            threshold: ì²­í‚¹ ì„ê³„ê°’ (ê¸°ë³¸ 1000ì)
            
        Returns:
            ì²­í‚¹ í•„ìš” ì—¬ë¶€
        """
        return len(text) > threshold if text else False
